{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loi de probabilité usuelles\n",
    "\n",
    "## Lois de probabilité discrètes\n",
    "\n",
    "### Loi de Bernoulli\n",
    "\n",
    "Considérons un processus aléatoire avec une variable aléatoire $X$ avec deux résultats possibles: succès ou échec.\n",
    "Cela correspond au tirage d'une pièce avec comme résultat pile ou face.\n",
    "Pour chaque résultat on associe une valeur 0 ou 1: pour un succès, $X=1$ et pour un échec, $X=0$.\n",
    "La probabilité d'obtenir un succès vaut $p$, celle d'obtenir un échec vaut donc $1-p$.\n",
    "La loi de probabilité, aussi appelé **loi de Bernoulli** de paramètre $p$, peut alors s'écrire:\n",
    "\\begin{equation}\n",
    "    P(x;p) = p^x(1-p)^{1-x}.\n",
    "\\end{equation}\n",
    "\n",
    "### Loi binomiale\n",
    "\n",
    "Si on répète $n$ fois une expérience de Bernoulli de façon indépendante et on compte le nombre de succès parmi ces $n$ tentatives, la probabilité d'obtenir $k$ succès (et donc $n-k$ échecs) est donnée par la loi binomiale.\n",
    "L'ordre des succès et échecs n'important pas, le nombre de combinaisons de $k$ succès parmi $n$ tentatives correspond au coefficient binomial $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ et la probabilité pour chaque combinaison vaut $p^k (1-p)^{n-k}$.\n",
    "La probabilité binomiale vaut donc:\n",
    "\\begin{equation}\n",
    "    P(k; n,p) = \\binom{n}{k}p^k (1-p)^{n-k}.\n",
    "\\end{equation}\n",
    "\n",
    "Le nombre moyen de succès $\\bar{k}$ parmi $n$ lancers est donné par:\n",
    "\\begin{equation}\n",
    "    \\bar{k} = \\mathbb{E}(k) = \\sum _{k=0}^N k\\times P(k; n,p) = n\\times p\n",
    "\\end{equation}\n",
    "et la variance du nombre de succès vaut:\n",
    "\\begin{equation}\n",
    "    \\sigma_k^2 = \\sum _{k=0}^N (k-\\bar{k})^2\\times P(k; n,p) = np(1-p).\n",
    "\\end{equation}\n",
    "\n",
    "Lorsqu'on souhaite extraire la probabilité $p$ de succès à partir de $k$ succès parmi $n$ essais, on peut utiliser la relation:\n",
    "\\begin{equation}\n",
    "    \\bar{p} = \\frac{k}{n}.\n",
    "\\end{equation}\n",
    "On voit donc que $\\mathbb{E}(\\bar{p}) = \\mathbb{E}(k/n) = p$ et que\n",
    "\\begin{equation}\n",
    "    \\sigma_{p} = \\frac{\\sigma_k}{n} = \\sqrt{\\frac{p(1-p)}{n}}.\n",
    "\\end{equation}\n",
    "\n",
    "### Loi de Poisson\n",
    "\n",
    "La loi de Poisson s'applique aux processus de comptage pour lesquels on compte le nombre de succès pendant une durée de temps fixée.\n",
    "Si on divise la durée de temps $\\Delta t$ en $N$ morceaux et on note $p$ la probabilité d'avoir un succès pendant cette période, le nombre moyen de succès est alors $Np$ et est donc proportionnel à $\\Delta t$.\n",
    "Cette approche est valide lorsque la probabilité de succès $p$ est faible (mais avec $\\lambda = Np$ non nul): dans ce cas, la probabilité d'observer $k$ succès pendant $N$ morceaux suit une loi binomiale:\n",
    "\\begin{equation}\n",
    "    P(k; n,p) = \\binom{n}{k}p^k (1-p)^{n-k} = \\frac{n!\\lambda ^k}{k!(n-k)!n^k}\\left( 1-\\frac{\\lambda}{n} \\right) ^{n-k}.\n",
    "\\end{equation}\n",
    "Or si $p$ est très petit devant 1, $N$ est très grand devant 1 et alors\n",
    "\\begin{equation}\n",
    "    \\frac{n!}{(n-k)!n^k} \\approx \\frac{n^k}{n^k} = 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left( 1-\\frac{\\lambda}{n} \\right) ^{n-k} \\approx \\left( 1-\\frac{\\lambda}{n} \\right) ^{n} \\approx \\left(\\exp -\\frac{\\lambda}{n}\\right)^n = e^{-\\lambda}.\n",
    "\\end{equation}\n",
    "On a donc la définition de la loi de probabilité de Poisson:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(k; \\lambda) = \\lim _{n\\rightarrow \\infty; p\\rightarrow 0} P(k; n,p) = \\frac{e^{-\\lambda}\\lambda ^k}{k!}.\n",
    "\\end{equation}\n",
    "\n",
    "La figure ci-dessous montre des distributions de Poisson avec plusieurs valeurs du paramètre $\\lambda$.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # importer maplotlib\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "from math import exp, sin, pi, cos\n",
    "\n",
    "# poissonList = np.random.poisson(5,1000000)\n",
    "xx = [val for val in range(0,20)]\n",
    "\n",
    "fig, ax = plt.subplots() # creation d'un plot\n",
    "line, = ax.plot(xx,poisson.pmf(xx, 3.),\"-\",marker=\"o\",label=\"Poisson(k,3)\") # plot de y1 vs x\n",
    "line, = ax.plot(xx,poisson.pmf(xx, 5.),\"-\",marker=\"o\",label=\"Poisson(k,5)\") # plot de y1 vs x\n",
    "line, = ax.plot(xx,poisson.pmf(xx, 10.),\"-\",marker=\"o\",label=\"Poisson(k,10)\") # plot de y1 vs x\n",
    "# n, bins, patches = plt.hist(poissonList, bins=np.arange(-0.5, 15.5, 1.), alpha=0.75, label=\"Gen. poisson\", density=True)\n",
    "ax.set_ylim(bottom=0.)\n",
    "ax.set_ylabel('k') # titre de l'axe x\n",
    "leg = ax.legend()   # creation de la légende"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "On remarque que la distribution n'est pas symétrique autour de la valeur de $\\lambda$, pour les valeurs de $\\lambda$ faibles; cependant à partir de $\\lambda=10$, on peut voir que la distribution devient relativement symétrique et de forme gaussienne (voir section [Loi normale ou loi de Gauss](#loi-normale-ou-loi-de-gauss)).\n",
    "\n",
    "Les caractéristiques de la loi de Poisson découle de celles de la loi binomiale.\n",
    "En utilisant le fait que $\\lambda = np$ et que $p \\ll 1$, le nombre moyen de succès $\\bar{k}$ est donné par:\n",
    "\\begin{equation}\n",
    "    \\bar{k} = \\mathbb{E}(k) = \\sum _{k=0}^{\\infty} k\\times P(k; \\lambda) = \\lambda\n",
    "\\end{equation}\n",
    "et la variance du nombre de succès vaut:\n",
    "\\begin{equation}\n",
    "    \\sigma_k^2 = \\sum _{k=0}^\\infty (k-\\bar{k})^2\\times P(k; \\lambda) = \\lambda.\n",
    "\\end{equation}\n",
    "L'écart-type d'une distribution de Poisson vaut donc $\\sqrt{\\sigma _k ^2} = \\sqrt{\\lambda}$.\n",
    "\n",
    "## Lois de probabilité continues\n",
    "\n",
    "### Loi uniforme\n",
    "\n",
    "La loi uniforme consiste à ce que toutes les intervalles de taille $\\mathrm{d}x$ sont équiprobables sur une gamme donnée.\n",
    "Plus précisément, la loi uniforme $f(x)$ sur l'intervalle $\\left[ a,b \\right]$ est définie par deux paramètres $a$ et $b$:\n",
    "\\begin{equation}\n",
    "    f(x;a,b) = \\frac{1}{b-a}\n",
    "\\end{equation}\n",
    "pour $a<x<b$ et nulle ailleurs.\n",
    "\n",
    "Sa moyenne vaut $\\mu = \\mathbb{E}[X] = \\frac{a+b}{2}$ et sa variance $\\sigma ^2 = \\frac{(b-a)^2}{12}$.\n",
    "\n",
    "### Loi normale ou loi de Gauss\n",
    "\n",
    "Cette loi est l'une des plus utilisées en statistique, car elle définit de nombreux processus aléatoires et est le résultat de plusieurs théorèmes majeurs comme le théorème centrale limite.\n",
    "Cette loi est définie par deux paramètres $\\mu$ et $\\sigma$:\n",
    "\\begin{equation}\n",
    "    f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}.\n",
    "\\end{equation}\n",
    "La moyenne de cette distribution est $\\mu$ et sa variance est $\\sigma ^2$.\n",
    "Notons que la médiane et le mode de cette distribution sont égaux à la moyenne.\n",
    "\n",
    "En sciences expérimentales, il est souvent plus simple de déterminer l'écart-type en utilisant la **largeur à mi-hauteur** (**FWHM** ou **Full Width Half Maximum**): elle est déterminée par la relation:\n",
    "\\begin{equation}\n",
    "    f(\\mu+\\mathrm{FWHM}/2; \\mu, \\sigma) = \\frac{1}{2} f(\\mu; \\mu, \\sigma) \\Rightarrow \\mathrm{FWHM} = 2\\sqrt{2\\ln 2}\\sigma \\approx 2.35 \\sigma.\n",
    "\\end{equation}\n",
    "\n",
    "La forme de la distribution gaussienne ainsi que la largeur à mi-hauteur sont représentées sur la figure ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # importer maplotlib\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "def gaussian(x, mu, sig):\n",
    "    return 1./(np.sqrt(2*3.1415)*sig)*np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "x = [ 0.1*i for i in range(-40,40) ] # liste d'abscisses de la figure\n",
    "y1 = [gaussian(val,0,1) for val in x]                          # calcul de y = f(x)\n",
    "\n",
    "fig, ax = plt.subplots() # creation d'un plot\n",
    "ax.plot(x,y1,\"-\",label=\"pdf(x)\") # plot de y1 vs x\n",
    "ax.set_xlabel('x') # titre de l'axe x\n",
    "plt.axvline(-2.35/2., 0, 10, c='orange')\n",
    "plt.axvline(2.35/2., 0, 10, c='orange')\n",
    "plt.axhline(0.5/(np.sqrt(2*3.14159)), -2.35/2., 2.35/2., ls=\"--\")\n",
    "leg = ax.legend()   # creation de la légende"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.0 (main, Oct 25 2022, 13:57:33) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
