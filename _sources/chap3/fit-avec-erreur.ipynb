{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(content:fit-with-error)=\n",
    "# Ajustement avec des données avec erreurs\n",
    "\n",
    "La minimisation du coefficient de détermination $R^2$ défini de façon \\textit{ad hoc} permet d'estimer la valeur des paramètres d'un modèle à partir d'un jeu de données.\n",
    "Nous allons présenter ici d'autres quantités ayant des motivations probabilistes, notamment lorsque l'on peut associer à chaque mesure une erreur.\n",
    "\n",
    "# Vraisemblance et moindre carrés\n",
    "\n",
    "Une quantité d'intérêt est la **fonction de vraisemblance** [^Likelihood].\n",
    "Considérons un jeu de données $\\{x_i\\} _{i=1...N}$.\n",
    "On définit la fonction de vraisemblance $\\mathcal{L}$ des données sachant un modèle comme étant la probabilité d'obtenir toutes les données $x_i$.\n",
    "Si $p(x_i;\\vec{\\theta})$ est la probabilité d'obtenir la mesure $x_i$ avec le jeu de paramètres $\\vec{\\theta}$, la vraisemblance s'écrira donc:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}(\\vec{\\theta}) = \\prod _i p(x_i;\\vec{\\theta}).\n",
    "$$ (vraisemblance)\n",
    "\n",
    "On suppose toujours que les mesures sont indépendantes.\n",
    "Si les données sont placées dans un histogramme, les $\\{x_i\\} _{i=1...N}$ sont le nombre de coups dans chaque bin, $p$ sera une probabilité suivant la loi de Poisson avec $\\lambda$ la valeur attendue du bin prédite par le modèle et comme $k$ le nombre d'évènements $x_i$ dans le bin.\n",
    "Le nombre de coups attendus $\\lambda$ peut lui même suivre un autre modèle et dépendre d'autres paramètres.\n",
    "\n",
    "Si les données sont des couples ou points $\\left(x_i, y_i\\right)$ avec des erreurs $\\sigma _i$ sur les $y_i$ et que l'on souhaite tester une relation entre $X$ et $Y$ (comme par exemple une dépendance linéaire $Y=aX+b$), les points de données sont dispersés *normalement* (selon une loi gaussienne) autour de la prédiction du modèle et la vraisemblance peut s'écrire comme:\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}(\\vec{\\theta}) = \\prod _i \\frac{1}{\\sqrt{2\\pi}\\sigma _i}\\exp \\left( -\\frac{\\left(y_i - f(x_i;\\vec{\\theta})\\right)^2}{2\\sigma _i^2} \\right),\n",
    "$$ (likelihood-gaussien)\n",
    "\n",
    "avec $\\theta$ les paramètres du modèle à tester.\n",
    "Dans ce cas, le nombre de paramètres $\\vec{\\theta}$ dépend de la complexité du modèle; les valeurs de $\\vec{\\theta}$ qui maximisent $\\mathcal{L}$ est le meilleur modèle par rapport aux données collectées.\n",
    "<!-- % Notons que cela ne veut pas dire que c'est les paramètres les plus vraisemblables pour ce modèle puisque $\\mathcal{L}$ ne contient pas la vraisemblance de ces paramètres a priori, c'est-à-dire avant l'obtention de ces mesures\\footnote{Pour cela, il faudrait donc aussi prendre en compte la probabilité de ces paramètres avant les mesures et les intégrer à la vraisemblance. -->\n",
    "<!-- % On peut utiliser le théorème de Bayes.}. -->\n",
    "Ici on vient de calculer les paramètres qui rendent les données obtenues les plus probables pour un modèle donné.\n",
    "\n",
    "On peut noter ici que puisque les probabilités $p(x_i;\\vec{\\theta})$ de l'équation {eq}`vraisemblance` sont plus petites que 1, la fonction de vraisemblance est plus petite que 1.\n",
    "Notamment, plus le nombre de points de données augmente, plus la vraisemblance est faible. \n",
    "\n",
    "Dans le cas où on considère un modèle gaussien comme dans l'équation {eq}`likelihood-gaussien`, on peut utiliser la fonction $\\ln$ pour simplifier le problème:\n",
    "\n",
    "$$ \n",
    "  \\chi ^2 = -2\\ln \\mathcal{L}(\\vec{\\theta}) = cste + \\sum _i\\frac{\\left(y_i - f(x_i;\\vec{\\theta})\\right)^2}{\\sigma _i^2}.\n",
    "$$ (def-chi2)\n",
    "\n",
    "Le terme $cste$ est bien une constante ne dépendant que des erreurs des points de données $\\sigma _i$.\n",
    "Le $\\chi^2$, aussi appelé **moindre carrés**, est la somme des écarts quadratiques entre le modèle et les données, pondérés par le carré inverse des incertitudes.\n",
    "Maximiser la vraisemblance {eq}`likelihood-gaussien` revient à minimiser le $\\chi ^2$ {eq}`def-chi2`.\n",
    "\n",
    "Notons que cela peut s'étendre aussi aux cas des distributions poissonniennes pour des données binnées puisque la distribution de Poisson tend vers une distribution gaussienne quand le nombre d'évènements attendu est \"grand\"; en pratique, pour $\\lambda \\geq 25$.\n",
    "\n",
    "## Extraction des meilleurs paramètres\n",
    "\n",
    "Trouver les meilleurs paramètres d'un modèle selon les données collectées revient à maximiser la vraisemblance ou bien minimiser le $\\chi ^2$.\n",
    "Cela peut se faire numériquement, par exemple, avec le module \\pyv{scipy.optimize} ou bien analytiquement: en généralisant pour des paramètres $\\vec{\\theta}$, le système d'équations à résoudre est\\footnote{Pour que ces paramètres minimisent bien le $\\chi^2$, il faut aussi vérifier que $\\left.\\frac{\\partial^2 \\chi ^2}{\\partial \\vec{\\theta}^2}\\right\\vert_{\\vec{\\theta}=\\hat{\\vec{\\theta}}} > 0$.}:\n",
    "\\begin{equation}\n",
    "  \\left.\\frac{\\partial \\chi ^2}{\\partial \\vec{\\theta}}\\right\\vert_{\\vec{\\theta}=\\hat{\\vec{\\theta}}} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "<!-- % Pour certains modèles, il est possible de trouver une expression analytique des meilleurs paramètres utilisant les valeurs des données. -->\n",
    "En particulier, pour des modèles qui dépendent linéairement de ses paramètres comme par exemple $f(x) = ax+b$ ou $f(x) = a\\sin x + be^x$, on peut résoudre le système:\n",
    "\n",
    "$$\n",
    "  \\left.\\frac{\\partial \\chi ^2}{\\partial a}\\right\\vert_{a=\\hat{a},b=\\hat{b}} = \\sum _i \\frac{\\partial f(x_i;\\hat{a},\\hat{b}) }{\\partial a}\\left( \\frac{y_i - f(x_i;\\hat{a},\\hat{b})}{\\sigma _i ^2} \\right) =  0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "  \\left.\\frac{\\partial \\chi ^2}{\\partial b}\\right\\vert_{a=\\hat{a},b=\\hat{b}} =  \\sum _i \\frac{\\partial f(x_i;\\hat{a},\\hat{b}) }{\\partial b}\\left( \\frac{y_i - f(x_i;\\hat{a},\\hat{b})}{\\sigma _i ^2} \\right) = 0\n",
    "$$\n",
    "\n",
    "pour $\\hat{a}$ et $\\hat{b}$ les paramètres optimaux du modèle.\n",
    "\n",
    "## Cas d'un ajustement linéaire\n",
    "\n",
    "Dans le cas fréquent où $f(x) = ax+b$, on peut résoudre le système en posant:\n",
    "\\begin{gather*}\n",
    "  A=\\sum_i \\frac{x_iy_i}{\\sigma _i ^2},\\,B=\\sum_i \\frac{x_i^2}{\\sigma _i ^2},\\\\\n",
    "  C=\\sum_i \\frac{x_i}{\\sigma _i ^2},\\,D=\\sum_i \\frac{y_i}{\\sigma _i ^2}\\, \\mathrm{et} \\, E=\\sum_i \\frac{1}{\\sigma _i ^2}.\n",
    "\\end{gather*}\n",
    "La solution est alors:\n",
    "\n",
    "$$\n",
    "  \\hat{a} = \\frac{AE-DC}{BE-C^2} \\, \\mathrm{et} \\, \\hat{b} = \\frac{DB-AC}{BE-C^2}.\n",
    "$$ (solana-lineaire)\n",
    "\n",
    "Dans le cas où $f(x) = ax$, l'expression du meilleur paramètre $\\hat{a}$ se simplifie comme:\n",
    "\n",
    "$$\n",
    "  \\hat{a} = \\frac{A}{B}.\n",
    "$$ (linearfit-analytical)\n",
    "\n",
    "(admon:linearfit_2)=\n",
    "```{admonition} Exemple: Ajustement linéaire simple -- suite\n",
    "En reprenant les données de l'exemple \\ref{ex:ajustement-lineaire-simple} ainsi que des erreurs associées, on peut calculer les valeurs optimales paramètres de pente et d'intersection à l'origine en utilisant\n",
    "\n",
    "-  une minimisation de $\\chi ^2$ donné par l'équation {eq}`def-chi2`,\n",
    "- les relations analytiques {eq}`solana-lineaire`.\n",
    "\n",
    "On peut voir que les résultats trouvés par les trois méthodes sont proches (ce qui est rassurant).\n",
    "En revanche, les paramètres sont différents que ceux trouvés par la méthode sans erreur de l'exemple \\ref{ex:ajustement-lineaire-simple}.\n",
    "Cela est du au fait que les points de données n'ont pas la même erreur, et donc le même poids.\n",
    "```\n",
    "\n",
    "[^Likelihood]: Likelihood en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import qexpy\n",
    "import qexpy.plotting as qplt\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n",
    "y = np.array([1.2, 1.6, 1.7, 2.2, 2.3, 2.4, 3.1, 3.3, 3.1, 3.7])\n",
    "err_y = np.array([0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.4, 0.4, 0.4, 0.4])\n",
    "\n",
    "def f(x,a,b):\n",
    "    return a*x+b\n",
    "\n",
    "# ajustement par curve_fit\n",
    "valeurs, cov = curve_fit(f, x, y,sigma=err_y)\n",
    "a_fit = valeurs[0]\n",
    "b_fit = valeurs[1]\n",
    "\n",
    "# ajustement par qexpy\n",
    "xmeas = qexpy.MeasurementArray(x, name=\"x\")\n",
    "ymeas = qexpy.MeasurementArray(y, error=err_y, name=\"y\")\n",
    "fig = qplt.plot(xmeas, ymeas, name='données', residuals=True)\n",
    "result = fig.fit(model=qexpy.FitModel.LINEAR)\n",
    "a_qexpy = result.params[0].value # extraction de a\n",
    "b_qexpy = result.params[1].value # extraction de b\n",
    "qplt.show()\n",
    "\n",
    "# méthode par les relations analytiques\n",
    "A = 0\n",
    "B = 0 \n",
    "C = 0 \n",
    "D = 0 \n",
    "E = 0\n",
    "for i in range(len(x)):\n",
    "    A += x[i]*y[i]/(err_y[i]*err_y[i])\n",
    "    B += x[i]*x[i]/(err_y[i]*err_y[i])\n",
    "    C += x[i]/(err_y[i]*err_y[i])\n",
    "    D += y[i]/(err_y[i]*err_y[i])\n",
    "    E += 1./(err_y[i]*err_y[i])\n",
    "a_ana = (A*E-D*C)/(B*E-C*C)\n",
    "b_ana = (D*B-A*C)/(B*E-C*C)\n",
    "\n",
    "print('''Résultats:\n",
    "Curve fit: a = {:.2f}; b = {:.2f}\n",
    "QExPy: a = {:.2f}; b = {:.2f}\n",
    "Analytique: a = {:.2f}; b = {:.2f}'''.format(a_fit, b_fit, a_qexpy, b_qexpy, a_ana, b_ana))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation des librairies externes permet de réaliser des ajustements plus complexes qui ne possèdent pas de solution analytique; il est cependant important de les comparer avec des relations analytiques (\"benchmarking\") afin de vérifier que les résultats données correspondent à ce que l'on attend.\n",
    "\n",
    "## Extraction de l'erreur sur un paramètre\n",
    "\n",
    "L'erreur sur un paramètre est défini en terme de probabilité d'obtenir la vraie valeur à partir des données mesurées: il faut intégrer la fonction de vraisemblance sur l'intervalle de confiance définie par la valeur moyenne $\\theta$ et l'erreur $\\sigma_{\\theta}$ sur les paramètres $\\left[\\theta-\\sigma_{\\theta},\\theta+\\sigma_{\\theta}\\right]$.\n",
    "En général, c'est un travail très complexe car la fonction de vraisemblance peut contenir des densité de probabilité complexe à écrire et intégrer; de plus, le nombre de dimensions à intégrer est égal au nombre de paramètres du modèle, rendant le calcul impraticable lorsque le nombre de paramètres est très grand.\n",
    "\n",
    "Cependant, pour des modèles simples comme des modèles linéaires avec des distributions gaussiennes des données, il est possible de calculer les erreurs sur les paramètres en utilisant le $\\chi ^2$ {eq}`def-chi2`.\n",
    "Dans le cas simple d'un ajustement linéaire $y=ax+b$, les erreurs sur les paramètres $a$ et $b$ sont définis par:\n",
    "\n",
    "$$\n",
    "\\Delta a \\,\\mathrm{tel\\,que\\,}\\, \\chi^{2}(a+\\Delta a, \\hat{b})=\\chi^{2}(a-\\Delta a, \\hat{b})=\\chi_{\\min }^{2}-2 \\ln (1-\\alpha),\n",
    "$$ (err-a)\n",
    "\n",
    "$$ \n",
    " \\Delta b \\,\\mathrm{tel\\,que\\,}\\, \\chi^{2}(\\hat{a}, b+\\Delta b)=\\chi^{2}(\\hat{a}, b-\\Delta b)=\\chi_{\\min }^{2}-2 \\ln (1-\\alpha),\n",
    "$$\n",
    "\n",
    "avec $\\alpha$ la probabilité souhaitée.\n",
    "Ces erreurs valent:\n",
    "\n",
    "$$\n",
    "    \\Delta a=\\frac{\\sqrt{-2 \\ln (1-\\alpha)}}{\\sqrt{B}}=\\frac{\\sqrt{-2 \\ln (1-\\alpha)}}{\\sqrt{\\sum \\frac{x_{1}^{2}}{\\left(\\Delta y_{1}\\right)^{2}}}},\n",
    "$$ (error-linearfit-a)\n",
    "\n",
    "$$ \n",
    "    \\Delta b=\\frac{\\sqrt{-2 \\ln (1-\\alpha)}}{\\sqrt{E}}=\\frac{\\sqrt{-2 \\ln (1-\\alpha)}}{\\sqrt{\\sum \\frac{1}{\\left(\\Delta y_{i}\\right)^{2}}}}.\n",
    "$$ (error-linearfit-b)\n",
    "\n",
    "Le terme $\\sqrt{-2 \\ln (1-\\alpha)}$ vaut:\n",
    "\n",
    "- pour une erreur définie \"à un sigma\", $\\alpha=68.3~\\%$, alors $\\sqrt{-2 \\ln (1-\\alpha)} = 1.52$,\n",
    "- pour une erreur définie \"à deux sigmas\", $\\alpha=95.4~\\%$, alors $\\sqrt{-2 \\ln (1-\\alpha)} = 2.49$,\n",
    "- pour une erreur définie \"à trois sigmas\", $\\alpha=99.7~\\%$, alors $\\sqrt{-2 \\ln (1-\\alpha)} = 3.44$.\n",
    "\n",
    "<!-- % Dans de distributions de données gaussiennes, les erreurs sur les paramètres du modèle sont estimées par la dérivée de la fonction de vraisemblance \\ref{eq:likelihood-gaussien} (et par conséquent au $\\chi ^2$ \\ref{eq:def-chi2}):\n",
    "% \\begin{equation}\n",
    "%     \\sigma _{\\theta}^2 = \\frac{1}{\\left(\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta}\\right)^2}= \\frac{-1}{\\frac{\\partial ^2 \\ln \\mathcal{L}}{\\partial \\theta^2}} = \\frac{1}{\\frac{1}{2}\\frac{\\partial ^2 \\chi ^2}{\\partial \\theta ^2}} .\n",
    "% \\end{equation}\n",
    "\n",
    "% \\todo[inline]{Ajouter le calcul des erreurs sur les paramètres d'ajustement} -->\n",
    "\n",
    "```{admonition} Exemple: Ajustement linéaire simple -- suite et fin\n",
    "En reprenant les données dans la table [](tab-linear-data), on peut calculer les erreurs en utilisant les relations {eq}`error-linearfit-a` et {eq}`error-linearfit-b`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n",
    "y = np.array([1.2, 1.6, 1.7, 2.2, 2.3, 2.4, 3.1, 3.3, 3.1, 3.7])\n",
    "err_y = np.array([0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.4, 0.4, 0.4, 0.4])\n",
    "from math import sqrt\n",
    "\n",
    "# relations analytiques\n",
    "B = 0 \n",
    "E = 0\n",
    "for i in range(len(x)):\n",
    "    B += x[i]*x[i]/(err_y[i]*err_y[i])\n",
    "    E += 1./(err_y[i]*err_y[i])\n",
    "\n",
    "a_error2 = 1.52/sqrt(B)\n",
    "b_error2 = 1.52/sqrt(E)\n",
    "\n",
    "print(\"Erreur sur a: {:.2f}\".format(a_error2))\n",
    "print(\"Erreur sur b: {:.2f}\".format(b_error2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur sur $a$ correspond à un intervalle à 68.3 % autour de la meilleure valeur de $a$ (ici, 0.27).\n",
    "De par sa définition {eq}`err-a`, cette erreur prend en compte la corrélation avec le coefficient $b$.\n",
    "Certains algorithmes comme QExPy donnent une valeur d'erreur sur les paramètres ajustés qui ne prend pas en compte les corrélations entre les paramètres, mais donnent la valeur de corrélation avec l'erreur.\n",
    "Il est donc important de toujours comparer le résultat et notamment les erreurs données par un algorithme ou une librairie au résultat donné par la méthode analytique afin de vérifier que les conventions sont identiques.\n",
    "\n",
    "(content:chi2-histo)=\n",
    "## Ajustement d'un histogramme\n",
    "\n",
    "Lorsque les données suivant une certaine loi de probabilité $f(x;\\vec{\\theta})$ sont rangées dans un histogramme, il est possible d'estimer les paramètres de cette densité de probabilité.\n",
    "Pour cela on construit la fonction de vraisemblance {eq}`vraisemblance` en supposant que le contenu de chaque classe $N_i$ est distribué selon une loi poissonienne autour de la valeur vraie $f(x_i;\\vec{\\theta})\\times \\delta x$ où $x_i$ représente le centre de la classe et $\\delta x$ la largeur de la classe: on a alors\n",
    "\\begin{equation}\n",
    "  \\mathcal{L}(\\vec{\\theta}) = \\prod _i \\frac{e^{-f(x_i;\\vec{\\theta}) \\delta x}\\left(f(x_i;\\vec{\\theta}) \\delta x\\right)^{N_i}}{N_i !}.\n",
    "\\end{equation}\n",
    "\n",
    "Dans le cas où le nombre de coups dans chaque classe est grand ($N_i>25$), on peut faire l'approximation de la densité de Poisson par une loi normale d'écart type $\\sigma _i = \\sqrt{N_i}$.\n",
    "La maximisation de la fonction de vraisemblance est alors la minimisation du $\\chi ^2$:\n",
    "\\begin{equation}\n",
    "    \\chi ^2 = \\sum _i\\frac{\\left(N_i - f(x_i;\\vec{\\theta})\\delta x\\right)^2}{N_i}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 (main, Oct 25 2022, 13:57:33) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
